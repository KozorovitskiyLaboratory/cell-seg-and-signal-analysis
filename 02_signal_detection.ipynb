{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSTRUCTION\n",
    "1. Run ALL the codes under 'image_projection', 'bigfish_utils', 'bigfish_img_processing', 'bigfish_signal_detection'\n",
    "2. NOW in the 'ACTUAL DETECTION STEP', modify the values depending on the directory you are working in & the file names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "### Projections 2-d ###\n",
    "\n",
    "def maximum_projection(image):\n",
    "    \"\"\"Project the z-dimension of an image, keeping the maximum intensity of\n",
    "    each yx pixel.\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        A 3-d image with shape (z, y, x).\n",
    "    Returns\n",
    "    -------\n",
    "    projected_image : np.ndarray\n",
    "        A 2-d image with shape (y, x).\n",
    "    \"\"\"\n",
    "    # project image along the z axis\n",
    "    projected_image = image.max(axis=0)\n",
    "\n",
    "    return projected_image\n",
    "\n",
    "\n",
    "def mean_projection(image, return_float=False):\n",
    "    \"\"\"Project the z-dimension of a image, computing the mean intensity of\n",
    "    each yx pixel.\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        A 3-d tensor with shape (z, y, x).\n",
    "    return_float : bool, default=False\n",
    "        Return a (potentially more accurate) float array.\n",
    "    Returns\n",
    "    -------\n",
    "    projected_image : np.ndarray\n",
    "        A 2-d image with shape (y, x).\n",
    "    \"\"\"\n",
    "    # project image along the z axis\n",
    "    if return_float:\n",
    "        projected_image = image.mean(axis=0)\n",
    "    else:\n",
    "        projected_image = image.mean(axis=0).astype(image.dtype)\n",
    "\n",
    "    return projected_image\n",
    "\n",
    "\n",
    "def median_projection(image):\n",
    "    \"\"\"Project the z-dimension of a image, computing the median intensity of\n",
    "    each yx pixel.\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        A 3-d image with shape (z, y, x).\n",
    "    Returns\n",
    "    -------\n",
    "    projected_image : np.ndarray\n",
    "        A 2-d image with shape (y, x).\n",
    "    \"\"\"\n",
    "    # project image along the z axis\n",
    "    projected_image = np.median(image, axis=0)\n",
    "    projected_image = projected_image.astype(image.dtype)\n",
    "\n",
    "    return projected_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigfish_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from skimage import io\n",
    "import inspect\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "\n",
    "def check_array(array, ndim=None, dtype=None, allow_nan=True):\n",
    "    \"\"\"\n",
    "    Full safety check of an array.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    array : np.ndarray\n",
    "        Array to check.\n",
    "    ndim : int or List[int]\n",
    "        Number of dimensions expected.\n",
    "    dtype : type or List[type]\n",
    "        Types expected.\n",
    "    allow_nan : bool\n",
    "        Allow NaN values or not.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    _ : bool\n",
    "        Assert if the array is well formatted.\n",
    "    \"\"\"\n",
    "    # check parameters\n",
    "    check_parameter(\n",
    "        array=np.ndarray,\n",
    "        ndim=(int, list, type(None)),\n",
    "        dtype=(type, list, type(None)),\n",
    "        allow_nan=bool)\n",
    "\n",
    "    # check the dtype\n",
    "    if dtype is not None:\n",
    "        _check_dtype_array(array, dtype)\n",
    "\n",
    "    # check the number of dimension\n",
    "    if ndim is not None:\n",
    "        _check_dim_array(array, ndim)\n",
    "\n",
    "    # check NaN\n",
    "    if not allow_nan:\n",
    "        _check_nan_array(array)\n",
    "\n",
    "    return True\n",
    "\n",
    "def _check_dtype_array(array, dtype):\n",
    "    \"\"\"\n",
    "    Check that a np.ndarray has the right dtype.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    array : np.ndarray\n",
    "        Array to check\n",
    "    dtype : type or List[type]\n",
    "        Type expected.\n",
    "    \"\"\"\n",
    "    # enlist the dtype expected\n",
    "    if isinstance(dtype, type):\n",
    "        dtype = [dtype]\n",
    "\n",
    "    # check the dtype of the array\n",
    "    error = True\n",
    "    for dtype_expected in dtype:\n",
    "        if array.dtype == dtype_expected:\n",
    "            error = False\n",
    "            break\n",
    "\n",
    "    if error:\n",
    "        raise TypeError(\"{0} is not supported yet. Use one of those dtypes \"\n",
    "                        \"instead: {1}.\".format(array.dtype, dtype))\n",
    "\n",
    "\n",
    "def _check_dim_array(array, ndim):\n",
    "    \"\"\"\n",
    "    Check that the array has the right number of dimensions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    array : np.ndarray\n",
    "        Array to check.\n",
    "    ndim : int or List[int]\n",
    "        Number of dimensions expected\n",
    "    \"\"\"\n",
    "    # enlist the number of expected dimensions\n",
    "    if isinstance(ndim, int):\n",
    "        ndim = [ndim]\n",
    "\n",
    "    # check the number of dimensions of the array\n",
    "    if array.ndim not in ndim:\n",
    "        raise ValueError(\"Array can't have {0} dimension(s). Expected \"\n",
    "                         \"dimensions are: {1}.\".format(array.ndim, ndim))\n",
    "\n",
    "\n",
    "def _check_nan_array(array):\n",
    "    \"\"\"\n",
    "    Check that the array does not have missing values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    array : np.ndarray\n",
    "        Array to check.\n",
    "    \"\"\"\n",
    "    # count nan\n",
    "    mask = np.isnan(array)\n",
    "    x = mask.sum()\n",
    "\n",
    "    # check the NaN values of the array\n",
    "    if x > 0:\n",
    "        raise ValueError(\"Array has {0} NaN values.\".format(x))\n",
    "\n",
    "\n",
    "def check_range_value(array, min_=None, max_=None):\n",
    "    \"\"\"\n",
    "    Check the support of the array.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    array : np.ndarray\n",
    "        Array to check.\n",
    "    min_ : int\n",
    "        Minimum value allowed.\n",
    "    max_ : int\n",
    "        Maximum value allowed.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    _ : bool\n",
    "        Assert if the array has the right range of values.\n",
    "    \"\"\"\n",
    "    # check lowest and highest bounds\n",
    "    if min_ is not None and array.min() < min_:\n",
    "        raise ValueError(\"The array should have a lower bound of {0}, but its \"\n",
    "                         \"minimum value is {1}.\".format(min_, array.min()))\n",
    "    if max_ is not None and array.max() > max_:\n",
    "        raise ValueError(\"The array should have an upper bound of {0}, but \"\n",
    "                         \"its maximum value is {1}.\".format(max_, array.max()))\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def check_parameter(**kwargs):\n",
    "    \"\"\"\n",
    "    Check dtype of the function's parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    kwargs : Type or Tuple[Type]\n",
    "        Map of each parameter with its expected dtype.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    _ : bool\n",
    "        Assert if the array is well formatted.\n",
    "    \"\"\"\n",
    "    # get the frame and the parameters of the function\n",
    "    frame = inspect.currentframe().f_back\n",
    "    _, _, _, values = inspect.getargvalues(frame)\n",
    "\n",
    "    # compare each parameter with its expected dtype\n",
    "    for arg in kwargs:\n",
    "        expected_dtype = kwargs[arg]\n",
    "        parameter = values[arg]\n",
    "        if not isinstance(parameter, expected_dtype):\n",
    "            actual = \"'{0}'\".format(type(parameter).__name__)\n",
    "            if isinstance(expected_dtype, tuple):\n",
    "                target = [\"'{0}'\".format(x.__name__) for x in expected_dtype]\n",
    "                target = \"(\" + \", \".join(target) + \")\"\n",
    "            else:\n",
    "                target = expected_dtype.__name__\n",
    "            raise TypeError(\"Parameter {0} should be a {1}. It is a {2} \"\n",
    "                            \"instead.\".format(arg, target, actual))\n",
    "\n",
    "    return True\n",
    "\n",
    "def read_image(path, sanity_check=False):\n",
    "    \"\"\"Read an image with ``png``, ``jpg``, ``jpeg``, ``tif`` or ``tiff`` extension.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path of the image to read.\n",
    "    sanity_check : bool\n",
    "        Check if the array returned fits with bigfish pipeline.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    image : ndarray, np.uint or np.int\n",
    "        Image read.\n",
    "    \"\"\"\n",
    "    # check path\n",
    "    check_parameter(\n",
    "        path=str,\n",
    "        sanity_check=bool)\n",
    "\n",
    "    # read image\n",
    "    image = io.imread(path)\n",
    "\n",
    "    # check the output image\n",
    "    if sanity_check:\n",
    "        check_array(\n",
    "            image,\n",
    "            dtype=[np.uint8, np.uint16, np.uint32, np.uint64,\n",
    "                   np.int8, np.int16, np.int32, np.int64,\n",
    "                   np.float16, np.float32, np.float64,\n",
    "                   bool],\n",
    "            ndim=[2, 3, 4, 5],\n",
    "            allow_nan=False)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigfish_img_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import skimage\n",
    "from skimage import img_as_ubyte\n",
    "from skimage import img_as_float32\n",
    "from skimage import img_as_float64\n",
    "from skimage import img_as_uint\n",
    "from skimage.exposure import rescale_intensity\n",
    "from skimage.transform import resize\n",
    "from sklearn.utils.fixes import parse_version\n",
    "from scipy.ndimage import gaussian_laplace\n",
    "\n",
    "\n",
    "def cast_img_uint8(tensor):\n",
    "    \"\"\"\n",
    "    Cast the image in np.uint8 and scale values between 0 and 255.\n",
    "    Negative values are not allowed as the skimage method ``img_as_ubyte`` would clip them to 0. \n",
    "    Positives values are scaled between 0 and 255,\n",
    "        except if they fit directly in 8 bit (in this case values are not modified).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tensor : np.ndarray\n",
    "        Image to cast.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tensor : np.ndarray, np.uint8\n",
    "        Image cast.\n",
    "    \"\"\"\n",
    "    # check tensor dtype\n",
    "    check_array(\n",
    "        tensor,\n",
    "        ndim=[2, 3, 4, 5],\n",
    "        dtype=[np.uint8, np.uint16, np.uint32, np.uint64,\n",
    "               np.int8, np.int16, np.int32, np.int64,\n",
    "               np.float32, np.float64])\n",
    "    if tensor.dtype in [np.float32, np.float64]:\n",
    "        check_range_value(tensor, min_=0, max_=1)\n",
    "    elif tensor.dtype in [np.int8, np.int16, np.int32, np.int64]:\n",
    "        check_range_value(tensor, min_=0)\n",
    "\n",
    "    if tensor.dtype == np.uint8:\n",
    "        return tensor\n",
    "\n",
    "    if (tensor.dtype in [np.uint16, np.uint32, np.uint64,\n",
    "                         np.int16, np.int32, np.int64]\n",
    "            and tensor.max() <= 255):\n",
    "        raise ValueError(\"Tensor values are between {0} and {1}. It fits in 8 \"\n",
    "                         \"bits and won't be scaled between 0 and 255. Use \"\n",
    "                         \"'tensor.astype(np.uint8)' instead.\"\n",
    "                         .format(tensor.min(), tensor.max()))\n",
    "\n",
    "    # cast tensor\n",
    "    if parse_version(skimage.__version__) < parse_version(\"0.16.0\"):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            tensor = img_as_ubyte(tensor)\n",
    "    else:\n",
    "        tensor = img_as_ubyte(tensor)\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def cast_img_uint16(tensor):\n",
    "    \"\"\"\n",
    "    Cast the data in np.uint16.\n",
    "    Negative values are not allowed as the skimage method ``img_as_uint`` would clip them to 0. \n",
    "    Positives values are scaled between 0 and 65535, \n",
    "        except if they fit directly in 16 bit (in this case values are not modified).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tensor : np.ndarray\n",
    "        Image to cast.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tensor : np.ndarray, np.uint16\n",
    "        Image cast.\n",
    "    \"\"\"\n",
    "    # check tensor dtype\n",
    "    check_array(\n",
    "        tensor,\n",
    "        ndim=[2, 3, 4, 5],\n",
    "        dtype=[np.uint8, np.uint16, np.uint32, np.uint64,\n",
    "               np.int8, np.int16, np.int32, np.int64,\n",
    "               np.float32, np.float64])\n",
    "    if tensor.dtype in [np.float32, np.float64]:\n",
    "        check_range_value(tensor, min_=0, max_=1)\n",
    "    elif tensor.dtype in [np.int8, np.int16, np.int32, np.int64]:\n",
    "        check_range_value(tensor, min_=0)\n",
    "\n",
    "    if tensor.dtype == np.uint16:\n",
    "        return tensor\n",
    "\n",
    "    if (tensor.dtype in [np.uint32, np.uint64, np.int32, np.int64]\n",
    "            and tensor.max() <= 65535):\n",
    "        raise ValueError(\"Tensor values are between {0} and {1}. It fits in \"\n",
    "                         \"16 bits and won't be scaled between 0 and 65535. \"\n",
    "                         \"Use 'tensor.astype(np.uint16)' instead.\"\n",
    "                         .format(tensor.min(), tensor.max()))\n",
    "\n",
    "    # cast tensor\n",
    "    if parse_version(skimage.__version__) < parse_version(\"0.16.0\"):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            tensor = img_as_uint(tensor)\n",
    "    else:\n",
    "        tensor = img_as_uint(tensor)\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def cast_img_float32(tensor):\n",
    "    \"\"\"\n",
    "    Cast the data in np.float32.\n",
    "    If the input data is in (unsigned) integer, the values are scaled between 0 and 1. \n",
    "    When converting from a np.float dtype, values are not modified.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tensor : np.ndarray\n",
    "        Image to cast.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tensor : np.ndarray, np.float32\n",
    "        image cast.\n",
    "    \"\"\"\n",
    "    # check tensor dtype\n",
    "    check_array(\n",
    "        tensor,\n",
    "        ndim=[2, 3, 4, 5],\n",
    "        dtype=[np.uint8, np.uint16, np.uint32, np.uint64,\n",
    "               np.int8, np.int16, np.int32, np.int64,\n",
    "               np.float32, np.float64])\n",
    "\n",
    "    # cast tensor\n",
    "    if parse_version(skimage.__version__) < parse_version(\"0.16.0\"):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            tensor = img_as_float32(tensor)\n",
    "    else:\n",
    "        tensor = img_as_float32(tensor)\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def cast_img_float64(tensor):\n",
    "    \"\"\"\n",
    "    Cast the data in np.float64.\n",
    "    If the input data is in (unsigned) integer, the values are scaled between 0 and 1. \n",
    "    When converting from a np.float dtype, values are not modified.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    tensor : np.ndarray\n",
    "        Tensor to cast.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tensor : np.ndarray, np.float64\n",
    "        Tensor cast.\n",
    "    \"\"\"\n",
    "    # check tensor dtype\n",
    "    check_array(\n",
    "        tensor,\n",
    "        ndim=[2, 3, 4, 5],\n",
    "        dtype=[np.uint8, np.uint16, np.uint32, np.uint64,\n",
    "               np.int8, np.int16, np.int32, np.int64,\n",
    "               np.float32, np.float64])\n",
    "\n",
    "    # cast tensor\n",
    "    tensor = img_as_float64(tensor)\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def log_filter(image, sigma):\n",
    "    \"\"\"\n",
    "    Apply a Laplacian of Gaussian (LoG) filter to a 2-d or 3-d image.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        Image with shape (z, y, x) or (y, x).\n",
    "    sigma : int, float, Tuple(float, int) or List(float, int)\n",
    "        Standard deviation used for the gaussian kernel (one for each dimension). \n",
    "        If it's a scalar, the same standard deviation is applied to every dimensions.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    image_filtered : np.ndarray\n",
    "        Filtered image.\n",
    "    \"\"\"\n",
    "    # check parameters\n",
    "    check_array(\n",
    "        image,\n",
    "        ndim=[2, 3],\n",
    "        dtype=[np.uint8, np.uint16, np.float32, np.float64])\n",
    "    check_parameter(sigma=(float, int, tuple, list))\n",
    "\n",
    "    # we cast the data in np.float to allow negative values\n",
    "    if image.dtype == np.uint8:\n",
    "        image_float = cast_img_float32(image)\n",
    "    elif image.dtype == np.uint16:\n",
    "        image_float = cast_img_float64(image)\n",
    "    else:\n",
    "        image_float = image\n",
    "\n",
    "    # check sigma\n",
    "    if isinstance(sigma, (tuple, list)):\n",
    "        if len(sigma) != image.ndim:\n",
    "            raise ValueError(\"'sigma' must be a scalar or a sequence with {0} \"\n",
    "                             \"elements.\".format(image.ndim))\n",
    "\n",
    "    # we apply LoG filter\n",
    "    image_filtered = gaussian_laplace(image_float, sigma=sigma)\n",
    "\n",
    "    # as the LoG filter makes the peaks in the original image appear as a\n",
    "    # reversed mexican hat, we inverse the result and clip negative values to 0\n",
    "    image_filtered = np.clip(-image_filtered, a_min=0, a_max=None)\n",
    "\n",
    "    # cast filtered image\n",
    "    if image.dtype == np.uint8:\n",
    "        image_filtered = cast_img_uint8(image_filtered)\n",
    "    elif image.dtype == np.uint16:\n",
    "        image_filtered = cast_img_uint16(image_filtered)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return image_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigfish_signal_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.ndimage as ndi\n",
    "from skimage.measure import regionprops\n",
    "from skimage.measure import label\n",
    "\n",
    "\n",
    "def get_object_radius_pixel(voxel_size_nm, object_radius_nm, ndim):\n",
    "    \"\"\"\n",
    "    Convert the object radius in pixel.\n",
    "    When the object considered is a spot this value can be interpreted as the standard deviation of the spot PSF (Point Spread Function), in pixel. \n",
    "    For any object modelled with a gaussian signal, this value can be interpreted as the standard deviation of the gaussian.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    voxel_size_nm : int, float, Tuple(int, float) or List(int, float)\n",
    "        Size of a voxel, in nanometer. One value per spatial dimension (zyx or yx dimensions). \n",
    "        If it's a scalar, the same value is applied to every dimensions.\n",
    "    object_radius_nm : int, float, Tuple(int, float) or List(int, float)\n",
    "        Radius of the object, in nanometer. One value per spatial dimension\n",
    "        (zyx or yx dimensions). If it's a scalar, the same radius is applied to\n",
    "        every dimensions.\n",
    "    ndim : int\n",
    "        Number of spatial dimension to consider.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    object_radius_px : Tuple[float]\n",
    "        Radius of the object in pixel, one element per dimension (zyx or yx dimensions).\n",
    "    \"\"\"\n",
    "    # check parameters\n",
    "    check_parameter(\n",
    "        voxel_size_nm=(int, float, tuple, list),\n",
    "        object_radius_nm=(int, float, tuple, list),\n",
    "        ndim=int)\n",
    "\n",
    "    voxel_size_nm = (voxel_size_nm,) * ndim\n",
    "    object_radius_nm = (object_radius_nm,) * ndim\n",
    "\n",
    "    # get radius in pixel\n",
    "    object_radius_px = [b / a for a, b in zip(voxel_size_nm, object_radius_nm)]\n",
    "    object_radius_px = tuple(object_radius_px)\n",
    "\n",
    "    return object_radius_px\n",
    "\n",
    "\n",
    "def get_breaking_point(x, y):\n",
    "    \"\"\"\n",
    "    Select the x-axis value where a L-curve has a kink (which is the most distant point from a L-curve from the segment [A, B]).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.array \n",
    "        X-axis values\n",
    "    y : np.array\n",
    "        Y-axis values \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    breaking_point : X-axis value at the kink location (float)\n",
    "    x : np.array \n",
    "        X-axis values \n",
    "    y : np.array\n",
    "        Y-axis values\n",
    "    \"\"\"\n",
    "    # check parameters\n",
    "    check_array(\n",
    "        x,\n",
    "        ndim=1,\n",
    "        dtype=[np.float32, np.float64, np.int32, np.int64])\n",
    "    check_array(\n",
    "        y,\n",
    "        ndim=1,\n",
    "        dtype=[np.float32, np.float64, np.int32, np.int64])\n",
    "\n",
    "    # select threshold where curve break\n",
    "    slope = (y[-1] - y[0]) / len(y)\n",
    "    y_grad = np.gradient(y)\n",
    "    m = list(y_grad >= slope)\n",
    "    j = m.index(False)\n",
    "    m = m[j:]\n",
    "    x = x[j:]\n",
    "    y = y[j:]\n",
    "    if True in m:\n",
    "        i = m.index(True)\n",
    "    else:\n",
    "        i = -1\n",
    "    breaking_point = float(x[i])\n",
    "\n",
    "    return breaking_point, x, y\n",
    "\n",
    "\n",
    "def moving_average(array, n):\n",
    "    \"\"\"\n",
    "    Compute a trailing average.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    array : np.ndarray\n",
    "        Array used to compute moving average \n",
    "    n :  int\n",
    "        Window width of the moving average\n",
    "       \n",
    "    Returns\n",
    "    -------\n",
    "    results : np.ndarray\n",
    "        Moving average values\n",
    "    \"\"\"\n",
    "    # check parameter\n",
    "    check_parameter(n=int)\n",
    "    check_array(array, ndim=1)\n",
    "\n",
    "    # compute moving average\n",
    "    cumsum = [0]\n",
    "    results = []\n",
    "    for i, x in enumerate(array, 1):\n",
    "        cumsum.append(cumsum[i-1] + x)\n",
    "        if i >= n:\n",
    "            ma = (cumsum[i] - cumsum[i - n]) / n\n",
    "            results.append(ma)\n",
    "    results = np.array(results)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def centered_moving_average(array, n):\n",
    "    \"\"\"\n",
    "    Compute a centered moving average.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    array : Array used to compute moving average (np.ndarray)\n",
    "        \n",
    "    n : Window width of the moving average (int)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    results : Centered moving average values (np.ndarray)\n",
    "    \"\"\"\n",
    "    # check parameter\n",
    "    check_parameter(n=int)\n",
    "    check_array(array, ndim=1)\n",
    "\n",
    "    # pad array to keep the same length and centered the outcome\n",
    "    if n % 2 == 0:\n",
    "        r = int(n / 2)\n",
    "        n += 1\n",
    "    else:\n",
    "        r = int((n - 1) / 2)\n",
    "    array_padded = np.pad(array, pad_width=r, mode=\"reflect\")\n",
    "\n",
    "    # compute centered moving average\n",
    "    results = moving_average(array_padded, n)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def detect_spots(\n",
    "        images,\n",
    "        threshold=None,\n",
    "        remove_duplicate=True,\n",
    "        return_threshold=False,\n",
    "        voxel_size=None,\n",
    "        spot_radius=None,\n",
    "        log_kernel_size=None,\n",
    "        minimum_distance=None):\n",
    "    \"\"\"\n",
    "    Apply LoG filter followed by a Local Maximum algorithm to detect spots in a 2-d or 3-d image.\n",
    "    1. Smooth the image with a LoG filter.\n",
    "    -> 2. We apply a multidimensional maximum filter.\n",
    "    -> 3. A pixel which has the same value in the original and filtered images is a local maximum.\n",
    "    -> 4. We remove local peaks under a threshold.\n",
    "    -> 5. We keep only one pixel coordinate per detected spot. \n",
    "        (So, even if it's a 3-d image, since we keep only one pixel coordinate per detected spot, we can ensure that there is no multiple countings of coordinates.)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    images : List[np.ndarray] or np.ndarray\n",
    "        Image (or list of images) with shape (z, y, x) or (y, x). \n",
    "        If several images are provided, the same threshold is applied.\n",
    "    threshold : int, float or None\n",
    "        A threshold to discriminate relevant spots from noisy blobs. \n",
    "        If None, optimal threshold is selected automatically (using the automated_threshold_setting() function.)\n",
    "        If several images are provided, one optimal threshold is selected for all the images.\n",
    "    remove_duplicate : bool\n",
    "        Remove potential duplicate coordinates for the same spots. \n",
    "    return_threshold : bool\n",
    "        Return the threshold used to detect spots.\n",
    "    voxel_size : int, float, Tuple(int, float), List(int, float) or None\n",
    "        Size of a voxel, in nanometer. \n",
    "        One value per spatial dimension (zyx or yx dimensions).\n",
    "        If it's a scalar, the same value is applied to every dimensions. \n",
    "        Not used if 'log_kernel_size' and 'minimum_distance' are provided.\n",
    "    spot_radius : int, float, Tuple(int, float), List(int, float) or None\n",
    "        Radius of the spot, in nanometer. \n",
    "        One value per spatial dimension (zyx or yx dimensions). \n",
    "        If it's a scalar, the same radius is applied to every dimensions. \n",
    "        Not used if 'log_kernel_size' and 'minimum_distance' are provided.\n",
    "    log_kernel_size : int, float, Tuple(int, float), List(int, float) or None\n",
    "        Size of the LoG kernel. \n",
    "        It equals the standard deviation (in pixels) used for the gaussian kernel (one for each dimension). \n",
    "        One value per spatial dimension (zyx or yx dimensions). \n",
    "        If it's a scalar, the same standard deviation is applied to every dimensions. \n",
    "        If None, we estimate it with the voxel size and spot radius.\n",
    "    minimum_distance : int, float, Tuple(int, float), List(int, float) or None\n",
    "        Minimum distance (in pixels) between two spots we want to be able to detect separately. \n",
    "        One value per spatial dimension (zyx or yx dimensions). \n",
    "        If it's a scalar, the same distance is applied to every dimensions. \n",
    "        If None, we estimate it with the voxel size and spot radius.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    spots : List[np.ndarray] or np.ndarray, np.int64\n",
    "        Coordinates (or list of coordinates) of the spots with shape\n",
    "        (nb_spots, 3) or (nb_spots, 2), for 3-d or 2-d images respectively.\n",
    "    threshold : int or float\n",
    "        Threshold used to discriminate spots from noisy blobs.\n",
    "    \"\"\"\n",
    "    # if one image is provided we enlist it\n",
    "    if not isinstance(images, list):\n",
    "        check_array(\n",
    "            images,\n",
    "            ndim=[2, 3],\n",
    "            dtype=[np.uint8, np.uint16, np.float32, np.float64])\n",
    "        ndim = images.ndim\n",
    "        images = [images]\n",
    "        is_list = False\n",
    "    else:\n",
    "        ndim = None\n",
    "        for i, image in enumerate(images):\n",
    "            check_array(\n",
    "                image,\n",
    "                ndim=[2, 3],\n",
    "                dtype=[np.uint8, np.uint16, np.float32, np.float64])\n",
    "            if i == 0:\n",
    "                ndim = image.ndim\n",
    "            else:\n",
    "                if ndim != image.ndim:\n",
    "                    raise ValueError(\"Provided images should have the same \"\n",
    "                                     \"number of dimensions.\")\n",
    "        is_list = True\n",
    "\n",
    "    # check consistency between parameters - detection with voxel size and spot radius\n",
    "    if (voxel_size is not None and spot_radius is not None\n",
    "            and log_kernel_size is None and minimum_distance is None):\n",
    "        if isinstance(voxel_size, (tuple, list)):\n",
    "            if len(voxel_size) != ndim:\n",
    "                raise ValueError(\"'voxel_size' must be a scalar or a sequence \"\n",
    "                                 \"with {0} elements.\".format(ndim))\n",
    "        else:\n",
    "            voxel_size = (voxel_size,) * ndim\n",
    "        if isinstance(spot_radius, (tuple, list)):\n",
    "            if len(spot_radius) != ndim:\n",
    "                raise ValueError(\"'spot_radius' must be a scalar or a \"\n",
    "                                 \"sequence with {0} elements.\".format(ndim))\n",
    "        else:\n",
    "            spot_radius = (spot_radius,) * ndim\n",
    "        log_kernel_size = get_object_radius_pixel(\n",
    "            voxel_size_nm=voxel_size,\n",
    "            object_radius_nm=spot_radius,\n",
    "            ndim=ndim)\n",
    "        minimum_distance = get_object_radius_pixel(\n",
    "            voxel_size_nm=voxel_size,\n",
    "            object_radius_nm=spot_radius,\n",
    "            ndim=ndim)\n",
    "\n",
    "    # # check consistency between parameters - detection with kernel size and minimal distance\n",
    "    # elif (voxel_size is None and spot_radius is None\n",
    "    #       and log_kernel_size is not None and minimum_distance is not None):\n",
    "    #     if isinstance(log_kernel_size, (tuple, list)):\n",
    "    #         if len(log_kernel_size) != ndim:\n",
    "    #             raise ValueError(\"'log_kernel_size' must be a scalar or a \"\n",
    "    #                              \"sequence with {0} elements.\".format(ndim))\n",
    "    #     else:\n",
    "    #         log_kernel_size = (log_kernel_size,) * ndim\n",
    "    #     if isinstance(minimum_distance, (tuple, list)):\n",
    "    #         if len(minimum_distance) != ndim:\n",
    "    #             raise ValueError(\"'minimum_distance' must be a scalar or a \"\n",
    "    #                              \"sequence with {0} elements.\".format(ndim))\n",
    "    #     else:\n",
    "    #         minimum_distance = (minimum_distance,) * ndim\n",
    "\n",
    "    # # check consistency between parameters - detection in priority with kernel size and minimal distance\n",
    "    # elif (voxel_size is not None and spot_radius is not None\n",
    "    #       and log_kernel_size is not None and minimum_distance is not None):\n",
    "    #     if isinstance(log_kernel_size, (tuple, list)):\n",
    "    #         if len(log_kernel_size) != ndim:\n",
    "    #             raise ValueError(\"'log_kernel_size' must be a scalar or a \"\n",
    "    #                              \"sequence with {0} elements.\".format(ndim))\n",
    "    #     else:\n",
    "    #         log_kernel_size = (log_kernel_size,) * ndim\n",
    "    #     if isinstance(minimum_distance, (tuple, list)):\n",
    "    #         if len(minimum_distance) != ndim:\n",
    "    #             raise ValueError(\"'minimum_distance' must be a scalar or a \"\n",
    "    #                              \"sequence with {0} elements.\".format(ndim))\n",
    "    #     else:\n",
    "    #         minimum_distance = (minimum_distance,) * ndim\n",
    "\n",
    "    # checking missing parameters\n",
    "    else:\n",
    "        raise ValueError(\"A pair of parameter ('voxel_size', \"\n",
    "                         \"'spot_radius') should be provided.\")\n",
    "\n",
    "    # detect spots\n",
    "    if return_threshold:\n",
    "        spots, threshold = _detect_spots_from_images(\n",
    "            images,\n",
    "            threshold=threshold,\n",
    "            remove_duplicate=remove_duplicate,\n",
    "            return_threshold=return_threshold,\n",
    "            log_kernel_size=log_kernel_size,\n",
    "            min_distance=minimum_distance)\n",
    "    else:\n",
    "        spots = _detect_spots_from_images(\n",
    "            images,\n",
    "            threshold=threshold,\n",
    "            remove_duplicate=remove_duplicate,\n",
    "            return_threshold=return_threshold,\n",
    "            log_kernel_size=log_kernel_size,\n",
    "            min_distance=minimum_distance)\n",
    " \n",
    "    # format results\n",
    "    if not is_list:\n",
    "        spots = spots[0]\n",
    "\n",
    "    # return threshold or not\n",
    "    if return_threshold:\n",
    "        return spots, threshold\n",
    "    else:\n",
    "        return spots\n",
    "\n",
    "def _detect_spots_from_images(\n",
    "        images,\n",
    "        threshold=None,\n",
    "        remove_duplicate=True,\n",
    "        return_threshold=False,\n",
    "        log_kernel_size=None,\n",
    "        min_distance=None):\n",
    "    \"\"\"\n",
    "    Apply LoG filter followed by a Local Maximum algorithm to detect spots in a 2-d or 3-d image.\n",
    "    1. Smooth the image with a LoG filter.\n",
    "    -> 2. We apply a multidimensional maximum filter.\n",
    "    -> 3. A pixel which has the same value in the original and filtered images is a local maximum.\n",
    "    -> 4, We remove local peaks under a threshold.\n",
    "    -> 5. We keep only one pixel coordinate per detected spot.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    images : List[np.ndarray]\n",
    "        List of images with shape (z, y, x) or (y, x). \n",
    "        The same threshold is applied to every images.\n",
    "    threshold : float or int\n",
    "        A threshold to discriminate relevant spots from noisy blobs. \n",
    "        If None, optimal threshold is selected automatically. \n",
    "        If several images are provided, one optimal threshold is selected for all the images.\n",
    "    remove_duplicate : bool\n",
    "        Remove potential duplicate coordinates for the same spots.\n",
    "    return_threshold : bool\n",
    "        Return the threshold used to detect spots.\n",
    "    log_kernel_size : int, float, Tuple(int, float), List(int, float) or None\n",
    "        Size of the LoG kernel. \n",
    "        It equals the standard deviation (in pixels) used for the gaussian kernel (one for each dimension). \n",
    "        One value per spatial dimension (zyx or yx dimensions). \n",
    "        If it's a scalar, the same standard deviation is applied to every dimensions. \n",
    "        If None, we estimate it with the voxel size and spot radius.\n",
    "    min_distance : int, float, Tuple(int, float), List(int, float) or None\n",
    "        Minimum distance (in pixels) between two spots we want to be able to detect separately. \n",
    "        One value per spatial dimension (zyx or yx dimensions). \n",
    "        If it's a scalar, the same distance is applied to every dimensions. \n",
    "        If None, we estimate it with the voxel size and spot radius.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    all_spots : List[np.ndarray], np.int64\n",
    "        List of spot coordinates with shape (nb_spots, 3) or (nb_spots, 2),\n",
    "        for 3-d or 2-d images respectively.\n",
    "    threshold : int or float\n",
    "        Threshold used to discriminate spots from noisy blobs.\n",
    "    \"\"\"\n",
    "    # initialization\n",
    "    n = len(images)\n",
    "\n",
    "    # apply LoG filter and find local maximum\n",
    "    images_filtered = []\n",
    "    pixel_values = []\n",
    "    masks = []\n",
    "    for image in images:\n",
    "        # filter image\n",
    "        image_filtered = log_filter(image, log_kernel_size)\n",
    "        images_filtered.append(image_filtered)\n",
    "\n",
    "        # get pixels value\n",
    "        pixel_values += list(image_filtered.ravel())\n",
    "\n",
    "        # find local maximum\n",
    "        mask_local_max = local_maximum_detection(image_filtered, min_distance)\n",
    "        masks.append(mask_local_max)\n",
    "\n",
    "    # get optimal threshold if necessary based on all the images\n",
    "    if threshold is None:\n",
    "\n",
    "        # get threshold values we want to test\n",
    "        thresholds = _get_candidate_thresholds(pixel_values)\n",
    "\n",
    "        # get spots count and its logarithm\n",
    "        all_value_spots = []\n",
    "        minimum_threshold = float(thresholds[0])\n",
    "        for i in range(n):\n",
    "            image_filtered = images_filtered[i]\n",
    "            mask_local_max = masks[i]\n",
    "            spots, mask_spots = spots_thresholding(\n",
    "                image_filtered, mask_local_max,\n",
    "                threshold=minimum_threshold,\n",
    "                remove_duplicate=False)\n",
    "            value_spots = image_filtered[mask_spots]\n",
    "            all_value_spots.append(value_spots)\n",
    "        all_value_spots = np.concatenate(all_value_spots)\n",
    "        thresholds, count_spots = _get_spot_counts(thresholds, all_value_spots)\n",
    "\n",
    "        # select threshold where the kink of the distribution is located\n",
    "        if count_spots.size > 0:\n",
    "            threshold, _, _ = get_breaking_point(thresholds, count_spots)\n",
    "\n",
    "    # detect spots\n",
    "    all_spots = []\n",
    "    for i in range(n):\n",
    "\n",
    "        # get images and masks\n",
    "        image_filtered = images_filtered[i]\n",
    "        mask_local_max = masks[i]\n",
    "\n",
    "        # detection\n",
    "        spots, _ = spots_thresholding(\n",
    "            image_filtered, mask_local_max, threshold, remove_duplicate)\n",
    "        all_spots.append(spots)\n",
    "\n",
    "    # return threshold or not\n",
    "    if return_threshold:\n",
    "        return all_spots, threshold\n",
    "    else:\n",
    "        return all_spots\n",
    "\n",
    "\n",
    "def local_maximum_detection(image, min_distance):\n",
    "    \"\"\"\n",
    "    Compute a mask to keep only local maximum, in 2-d and 3-d.\n",
    "    1. We apply a multidimensional maximum filter.\n",
    "    -> 2. A pixel which has the same value in the original and filtered images is a local maximum.\n",
    "    Several connected pixels can have the same value. \n",
    "    In such a case, the local maximum is not unique.\n",
    "    In order to make the detection robust, it should be applied to a filtered image (using :func:`bigfish.stack.log_filter` for example).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        Image to process with shape (z, y, x) or (y, x).\n",
    "    min_distance : int, float, Tuple(int, float), List(int, float)\n",
    "        Minimum distance (in pixels) between two spots we want to be able to detect separately. \n",
    "        One value per spatial dimension (zyx or yx dimensions). \n",
    "        If it's a scalar, the same distance is applied to every dimension.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mask : np.ndarray, bool\n",
    "        Mask with shape (z, y, x) or (y, x) indicating the local peaks.\n",
    "    \"\"\"\n",
    "    # check parameters\n",
    "    check_array(\n",
    "        image,\n",
    "        ndim=[2, 3],\n",
    "        dtype=[np.uint8, np.uint16, np.float32, np.float64])\n",
    "    check_parameter(min_distance=(int, float, tuple, list))\n",
    "\n",
    "    # compute the kernel size (centered around our pixel because it is uneven)\n",
    "    if isinstance(min_distance, (tuple, list)):\n",
    "        if len(min_distance) != image.ndim:\n",
    "            raise ValueError(\n",
    "                \"'min_distance' should be a scalar or a sequence with one \"\n",
    "                \"value per dimension. Here the image has {0} dimensions and \"\n",
    "                \"'min_distance' {1} elements.\".format(image.ndim,\n",
    "                                                      len(min_distance)))\n",
    "    else:\n",
    "        min_distance = (min_distance,) * image.ndim\n",
    "    min_distance = np.ceil(min_distance).astype(image.dtype)\n",
    "    kernel_size = 2 * min_distance + 1\n",
    "\n",
    "    # apply maximum filter to the original image\n",
    "    image_filtered = ndi.maximum_filter(image, size=kernel_size)\n",
    "\n",
    "    # we keep the pixels with the same value before and after the filtering\n",
    "    mask = image == image_filtered\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def spots_thresholding(\n",
    "        image,\n",
    "        mask_local_max,\n",
    "        threshold,\n",
    "        remove_duplicate=True):\n",
    "    \"\"\"\n",
    "    Filter detected spots and get coordinates of the remaining spots.\n",
    "    In order to make the thresholding robust, it should be applied to a filtered image \n",
    "        (here, we are applying to the image filtered using log_filter() function). \n",
    "    If the local maximum is not unique (it can happen if connected pixels have the same value), \n",
    "        a connected component algorithm is applied to keep only one coordinate per spot.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        Image with shape (z, y, x) or (y, x).\n",
    "    mask_local_max : np.ndarray, bool\n",
    "        Mask with shape (z, y, x) or (y, x) indicating the local peaks.\n",
    "    threshold : float, int or None\n",
    "        A threshold to discriminate relevant spots from noisy blobs. \n",
    "        If None, detection is aborted with a warning.\n",
    "    remove_duplicate : bool\n",
    "        Remove potential duplicate coordinates for the same spots. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    spots : np.ndarray, np.int64\n",
    "        Coordinate of the local peaks with shape (nb_peaks, 3) or\n",
    "        (nb_peaks, 2) for 3-d or 2-d images respectively.\n",
    "    mask : np.ndarray, bool\n",
    "        Mask with shape (z, y, x) or (y, x) indicating the spots.\n",
    "    \"\"\"\n",
    "    # check parameters\n",
    "    check_array(\n",
    "        image,\n",
    "        ndim=[2, 3],\n",
    "        dtype=[np.uint8, np.uint16, np.float32, np.float64])\n",
    "    check_array(\n",
    "        mask_local_max,\n",
    "        ndim=[2, 3],\n",
    "        dtype=bool)\n",
    "    check_parameter(\n",
    "        threshold=(float, int, type(None)),\n",
    "        remove_duplicate=bool)\n",
    "\n",
    "    if threshold is None:\n",
    "        mask = np.zeros_like(image, dtype=bool)\n",
    "        spots = np.array([], dtype=np.int64).reshape((0, image.ndim))\n",
    "        warnings.warn(\"No spots were detected (threshold is {0}).\"\n",
    "                      .format(threshold),\n",
    "                      UserWarning)\n",
    "        return spots, mask\n",
    "\n",
    "    # remove peak with a low intensity\n",
    "    mask = (mask_local_max & (image > threshold))\n",
    "    if mask.sum() == 0:\n",
    "        spots = np.array([], dtype=np.int64).reshape((0, image.ndim))\n",
    "        return spots, mask\n",
    "\n",
    "    # make sure we detect only one coordinate per spot\n",
    "    if remove_duplicate:\n",
    "        # when several pixels are assigned to the same spot, keep the centroid\n",
    "        cc = label(mask)\n",
    "        local_max_regions = regionprops(cc)\n",
    "        spots = []\n",
    "        for local_max_region in local_max_regions:\n",
    "            spot = np.array(local_max_region.centroid)\n",
    "            spots.append(spot)\n",
    "        spots = np.stack(spots).astype(np.int64)\n",
    "\n",
    "        # built mask again\n",
    "        mask = np.zeros_like(mask)\n",
    "        mask[spots[:, 0], spots[:, 1]] = True\n",
    "\n",
    "    else:\n",
    "        # get peak coordinates\n",
    "        spots = np.nonzero(mask)\n",
    "        spots = np.column_stack(spots)\n",
    "\n",
    "    # case where no spots were detected\n",
    "    if spots.size == 0:\n",
    "        warnings.warn(\"No spots were detected (threshold is {0}).\"\n",
    "                      .format(threshold),\n",
    "                      UserWarning)\n",
    "\n",
    "    return spots, mask\n",
    "\n",
    "\n",
    "def automated_threshold_setting(image, mask_local_max):\n",
    "    \"\"\"\n",
    "    Automatically set the optimal threshold to detect spots.\n",
    "    In order to make the thresholding robust, it should be applied to a filtered image. \n",
    "    The optimal threshold is selected based on the spots distribution. \n",
    "    The latter should have an elbow curve discriminating a fast decreasing stage from a more stable one (a plateau).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.ndarray\n",
    "        Image with shape (z, y, x) or (y, x).\n",
    "    mask_local_max : np.ndarray, bool\n",
    "        Mask with shape (z, y, x) or (y, x) indicating the local peaks.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    optimal_threshold : int\n",
    "        Optimal threshold to discriminate spots from noisy blobs.\n",
    "    \"\"\"\n",
    "    # check parameters\n",
    "    check_array(\n",
    "        image,\n",
    "        ndim=[2, 3],\n",
    "        dtype=[np.uint8, np.uint16, np.float32, np.float64])\n",
    "    check_array(\n",
    "        mask_local_max,\n",
    "        ndim=[2, 3],\n",
    "        dtype=bool)\n",
    "\n",
    "    # get threshold values we want to test\n",
    "    thresholds = _get_candidate_thresholds(image.ravel())\n",
    "\n",
    "    # get spots count and its logarithm\n",
    "    first_threshold = float(thresholds[0])\n",
    "    spots, mask_spots = spots_thresholding(\n",
    "        image, mask_local_max, first_threshold, remove_duplicate=False)\n",
    "    value_spots = image[mask_spots]\n",
    "    thresholds, count_spots = _get_spot_counts(thresholds, value_spots)\n",
    "\n",
    "    # select threshold where the break of the distribution is located\n",
    "    if count_spots.size > 0:\n",
    "        optimal_threshold, _, _ = get_breaking_point(thresholds, count_spots)\n",
    "\n",
    "    # case where no spots were detected\n",
    "    else:\n",
    "        optimal_threshold = None\n",
    "\n",
    "    return optimal_threshold\n",
    "\n",
    "\n",
    "def _get_candidate_thresholds(pixel_values):\n",
    "    \"\"\"\n",
    "    Choose the candidate thresholds to test for the spot detection.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pixel_values : np.ndarray\n",
    "        Pixel intensity values of the image.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    thresholds : np.ndarray, np.float64\n",
    "        Candidate threshold values.\n",
    "    \"\"\"\n",
    "    # choose appropriate thresholds candidate\n",
    "    start_range = 0\n",
    "    end_range = int(np.percentile(pixel_values, 99.9999))\n",
    "    if end_range < 100:\n",
    "        thresholds = np.linspace(start_range, end_range, num=100)\n",
    "    else:\n",
    "        thresholds = [i for i in range(start_range, end_range + 1)]\n",
    "    thresholds = np.array(thresholds)\n",
    "\n",
    "    return thresholds\n",
    "\n",
    "\n",
    "def _get_spot_counts(thresholds, value_spots):\n",
    "    \"\"\"\n",
    "    Compute and format the spots count function for different thresholds.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    thresholds : np.ndarray, np.float64\n",
    "        Candidate threshold values.\n",
    "    value_spots : np.ndarray\n",
    "        Pixel intensity values of all spots.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    thresholds : np.ndarray, np.float64\n",
    "        Candidate threshold values.\n",
    "    count_spots : np.ndarray, np.float64\n",
    "        Spots count function (log scale).\n",
    "    \"\"\"\n",
    "    # count spots for each threshold\n",
    "    count_spots = np.log([np.count_nonzero(value_spots > t)\n",
    "                          for t in thresholds])\n",
    "    count_spots = centered_moving_average(count_spots, n=5)\n",
    "\n",
    "    # the tail of the curve unnecessarily flatten the slop\n",
    "    count_spots = count_spots[count_spots > 2]\n",
    "    thresholds = thresholds[:count_spots.size]\n",
    "\n",
    "    return thresholds, count_spots\n",
    "\n",
    "\n",
    "def get_elbow_values(\n",
    "        images,\n",
    "        voxel_size=None,\n",
    "        spot_radius=None,\n",
    "        log_kernel_size=None,\n",
    "        minimum_distance=None):\n",
    "    \"\"\"\n",
    "    Get values to plot the elbow curve used to automatically set the threshold to detect spots.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    images : List[np.ndarray] or np.ndarray\n",
    "        Image (or list of images) with shape (z, y, x) or (y, x). If several\n",
    "        images are provided, the same threshold is applied.\n",
    "    voxel_size : int, float, Tuple(int, float), List(int, float) or None\n",
    "        Size of a voxel, in nanometer. One value per spatial dimension (zyx or\n",
    "        yx dimensions). If it's a scalar, the same value is applied to every\n",
    "        dimensions. Not used if 'log_kernel_size' and 'minimum_distance' are\n",
    "        provided.\n",
    "    spot_radius : int, float, Tuple(int, float), List(int, float) or None\n",
    "        Radius of the spot, in nanometer. One value per spatial dimension (zyx\n",
    "        or yx dimensions). If it's a scalar, the same radius is applied to\n",
    "        every dimensions. Not used if 'log_kernel_size' and 'minimum_distance'\n",
    "        are provided.\n",
    "    log_kernel_size : int, float, Tuple(int, float), List(int, float) or None\n",
    "        Size of the LoG kernel. It equals the standard deviation (in pixels)\n",
    "        used for the gaussian kernel (one for each dimension). One value per\n",
    "        spatial dimension (zyx or yx dimensions). If it's a scalar, the same\n",
    "        standard deviation is applied to every dimensions. If None, we estimate\n",
    "        it with the voxel size and spot radius.\n",
    "    minimum_distance : int, float, Tuple(int, float), List(int, float) or None\n",
    "        Minimum distance (in pixels) between two spots we want to be able to\n",
    "        detect separately. One value per spatial dimension (zyx or yx\n",
    "        dimensions). If it's a scalar, the same distance is applied to every\n",
    "        dimensions. If None, we estimate it with the voxel size and spot\n",
    "        radius.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    thresholds : np.ndarray, np.float64\n",
    "        Candidate threshold values.\n",
    "    count_spots : np.ndarray, np.float64\n",
    "        Spots count (log scale).\n",
    "    threshold : float or None\n",
    "        Threshold automatically set.\n",
    "    \"\"\"\n",
    "    # check parameters\n",
    "    check_parameter(\n",
    "        voxel_size=(int, float, tuple, list, type(None)),\n",
    "        spot_radius=(int, float, tuple, list, type(None)),\n",
    "        log_kernel_size=(int, float, tuple, list, type(None)),\n",
    "        minimum_distance=(int, float, tuple, list, type(None)))\n",
    "\n",
    "    # if one image is provided we enlist it\n",
    "    if not isinstance(images, list):\n",
    "        check_array(\n",
    "            images,\n",
    "            ndim=[2, 3],\n",
    "            dtype=[np.uint8, np.uint16, np.float32, np.float64])\n",
    "        ndim = images.ndim\n",
    "        images = [images]\n",
    "        n = 1\n",
    "    else:\n",
    "        ndim = None\n",
    "        for i, image in enumerate(images):\n",
    "            check_array(\n",
    "                image,\n",
    "                ndim=[2, 3],\n",
    "                dtype=[np.uint8, np.uint16, np.float32, np.float64])\n",
    "            if i == 0:\n",
    "                ndim = image.ndim\n",
    "            else:\n",
    "                if ndim != image.ndim:\n",
    "                    raise ValueError(\"Provided images should have the same \"\n",
    "                                     \"number of dimensions.\")\n",
    "        n = len(images)\n",
    "\n",
    "    # check consistency between parameters - detection with voxel size and\n",
    "    # spot radius\n",
    "    if (voxel_size is not None and spot_radius is not None\n",
    "            and log_kernel_size is None and minimum_distance is None):\n",
    "        if isinstance(voxel_size, (tuple, list)):\n",
    "            if len(voxel_size) != ndim:\n",
    "                raise ValueError(\n",
    "                    \"'voxel_size' must be a scalar or a sequence \"\n",
    "                    \"with {0} elements.\".format(ndim))\n",
    "        else:\n",
    "            voxel_size = (voxel_size,) * ndim\n",
    "        if isinstance(spot_radius, (tuple, list)):\n",
    "            if len(spot_radius) != ndim:\n",
    "                raise ValueError(\"'spot_radius' must be a scalar or a \"\n",
    "                                 \"sequence with {0} elements.\".format(ndim))\n",
    "        else:\n",
    "            spot_radius = (spot_radius,) * ndim\n",
    "\n",
    "        log_kernel_size = get_object_radius_pixel(\n",
    "            voxel_size_nm=voxel_size,\n",
    "            object_radius_nm=spot_radius,\n",
    "            ndim=ndim)\n",
    "        minimum_distance = get_object_radius_pixel(\n",
    "            voxel_size_nm=voxel_size,\n",
    "            object_radius_nm=spot_radius,\n",
    "            ndim=ndim)\n",
    "\n",
    "    # check consistency between parameters - detection with kernel size and\n",
    "    # minimal distance\n",
    "    elif (voxel_size is None and spot_radius is None\n",
    "          and log_kernel_size is not None and minimum_distance is not None):\n",
    "        if isinstance(log_kernel_size, (tuple, list)):\n",
    "            if len(log_kernel_size) != ndim:\n",
    "                raise ValueError(\"'log_kernel_size' must be a scalar or a \"\n",
    "                                 \"sequence with {0} elements.\".format(ndim))\n",
    "        else:\n",
    "            log_kernel_size = (log_kernel_size,) * ndim\n",
    "        if isinstance(minimum_distance, (tuple, list)):\n",
    "            if len(minimum_distance) != ndim:\n",
    "                raise ValueError(\n",
    "                    \"'minimum_distance' must be a scalar or a \"\n",
    "                    \"sequence with {0} elements.\".format(ndim))\n",
    "        else:\n",
    "            minimum_distance = (minimum_distance,) * ndim\n",
    "\n",
    "    # check consistency between parameters - detection in priority with kernel\n",
    "    # size and minimal distance\n",
    "    elif (voxel_size is not None and spot_radius is not None\n",
    "          and log_kernel_size is not None and minimum_distance is not None):\n",
    "        if isinstance(log_kernel_size, (tuple, list)):\n",
    "            if len(log_kernel_size) != ndim:\n",
    "                raise ValueError(\"'log_kernel_size' must be a scalar or a \"\n",
    "                                 \"sequence with {0} elements.\".format(ndim))\n",
    "        else:\n",
    "            log_kernel_size = (log_kernel_size,) * ndim\n",
    "        if isinstance(minimum_distance, (tuple, list)):\n",
    "            if len(minimum_distance) != ndim:\n",
    "                raise ValueError(\n",
    "                    \"'minimum_distance' must be a scalar or a \"\n",
    "                    \"sequence with {0} elements.\".format(ndim))\n",
    "        else:\n",
    "            minimum_distance = (minimum_distance,) * ndim\n",
    "\n",
    "    # missing parameters\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"One of the two pairs of parameters ('voxel_size', \"\n",
    "            \"'spot_radius') or ('log_kernel_size', \"\n",
    "            \"'minimum_distance') should be provided.\")\n",
    "\n",
    "    # apply LoG filter and find local maximum\n",
    "    images_filtered = []\n",
    "    pixel_values = []\n",
    "    masks = []\n",
    "    for image in images:\n",
    "        # filter image\n",
    "        image_filtered = log_filter(image, log_kernel_size)\n",
    "        images_filtered.append(image_filtered)\n",
    "\n",
    "        # get pixels value\n",
    "        pixel_values += list(image_filtered.ravel())\n",
    "\n",
    "        # find local maximum\n",
    "        mask_local_max = local_maximum_detection(\n",
    "            image_filtered, minimum_distance)\n",
    "        masks.append(mask_local_max)\n",
    "\n",
    "    # get threshold values we want to test\n",
    "    thresholds = _get_candidate_thresholds(pixel_values)\n",
    "\n",
    "    # get spots count and its logarithm\n",
    "    all_value_spots = []\n",
    "    minimum_threshold = float(thresholds[0])\n",
    "    for i in range(n):\n",
    "        image_filtered = images_filtered[i]\n",
    "        mask_local_max = masks[i]\n",
    "        spots, mask_spots = spots_thresholding(\n",
    "            image_filtered, mask_local_max,\n",
    "            threshold=minimum_threshold,\n",
    "            remove_duplicate=False)\n",
    "        value_spots = image_filtered[mask_spots]\n",
    "        all_value_spots.append(value_spots)\n",
    "    all_value_spots = np.concatenate(all_value_spots)\n",
    "    thresholds, count_spots = _get_spot_counts(\n",
    "        thresholds, all_value_spots)\n",
    "\n",
    "    # select threshold where the kink of the distribution is located\n",
    "    if count_spots.size > 0:\n",
    "        threshold, _, _ = get_breaking_point(thresholds, count_spots)\n",
    "    else:\n",
    "        threshold = None\n",
    "\n",
    "    return thresholds, count_spots, threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACTUAL DETECTION STEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard-code the paths of our input and output directories\n",
    "path_input = \"/Users/laurenhyoseoyoon/Dropbox/2022_SUMMER/Kozorovitskiy-Lab/3d-pipeline/series5\"\n",
    "path_output = \"/Users/laurenhyoseoyoon/Dropbox/2022_SUMMER/Kozorovitskiy-Lab/3d-pipeline/series5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_gfp = os.path.join(path_input, \"Series5_C2_gfp.tif\")\n",
    "gfp = read_image(path_gfp)\n",
    "print(\"gfp channel\")\n",
    "print(\"\\r shape: {0}\".format(gfp.shape))\n",
    "print(\"\\r dtype: {0}\".format(gfp.dtype), \"\\n\")\n",
    "\n",
    "path_oxt = os.path.join(path_input, \"Series5_C3_oxt.tif\")\n",
    "oxt = read_image(path_oxt)\n",
    "print(\"oxt channel\")\n",
    "print(\"\\r shape: {0}\".format(oxt.shape))\n",
    "print(\"\\r dtype: {0}\".format(oxt.dtype), \"\\n\")\n",
    "\n",
    "path_tdT = os.path.join(path_input, \"Series5_C4_tdT.tif\")\n",
    "tdT = read_image(path_tdT)\n",
    "print(\"da channel\")\n",
    "print(\"\\r shape: {0}\".format(tdT.shape))\n",
    "print(\"\\r dtype: {0}\".format(tdT.dtype), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oxt_max = maximum_projection(oxt)\n",
    "oxt_mean = mean_projection(oxt, return_float=False)\n",
    "oxt_median = median_projection(oxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gfp_max = maximum_projection(gfp)\n",
    "gfp_mean = mean_projection(gfp, return_float=False)\n",
    "gfp_median = median_projection(gfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdT_max = maximum_projection(tdT)\n",
    "tdT_mean = mean_projection(tdT, return_float=False)\n",
    "tdT_median = median_projection(tdT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Detect OXT spots\n",
    "spots_oxt_2d, threshold = detect_spots(\n",
    "    images=oxt_mean, \n",
    "    return_threshold=True, \n",
    "    voxel_size=(103, 103),  # in nanometer (one value per dimension 3d(z,y,x) OR 2d(y,x))\n",
    "    spot_radius=(150, 150))  # in nanometer (one value per dimension 3d(z,y,x) OR 2d(y,x))\n",
    "print(\"detected spots\")\n",
    "print(\"\\r shape: {0}\".format(spots_oxt_2d.shape))\n",
    "print(\"\\r dtype: {0}\".format(spots_oxt_2d.dtype))\n",
    "print(\"\\r threshold: {0}\".format(threshold))\n",
    "\n",
    "## Saving the result\n",
    "np.savetxt('spots_oxt_2d.txt', spots_oxt_2d)\n",
    "    # (y, x) coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Detect tdT spots\n",
    "spots_tdT_2d, threshold = detect_spots(\n",
    "    images=tdT_mean, \n",
    "    return_threshold=True, \n",
    "    voxel_size=(103, 103),  \n",
    "    spot_radius=(150, 150))  \n",
    "print(\"detected spots\")\n",
    "print(\"\\r shape: {0}\".format(spots_tdT_2d.shape))\n",
    "print(\"\\r dtype: {0}\".format(spots_tdT_2d.dtype))\n",
    "print(\"\\r threshold: {0}\".format(threshold))\n",
    "\n",
    "## Saving the result\n",
    "np.savetxt('spots_tdT_2d.txt', spots_tdT_2d)\n",
    "    # (y, x) coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Detect GFP spots\n",
    "spots_gfp_2d, threshold = detect_spots(\n",
    "    images=gfp_mean, \n",
    "    return_threshold=True, \n",
    "    voxel_size=(103, 103),  \n",
    "    spot_radius=(150, 150))  \n",
    "print(\"detected spots\")\n",
    "print(\"\\r shape: {0}\".format(spots_gfp_2d.shape))\n",
    "print(\"\\r dtype: {0}\".format(spots_gfp_2d.dtype))\n",
    "print(\"\\r threshold: {0}\".format(threshold))\n",
    "\n",
    "## Saving the result\n",
    "np.savetxt('spots_gfp_2d.txt', spots_gfp_2d)\n",
    "    # (y, x) coordinates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f30eb7ee89319513ec763dad7f8e740f82ab3227361319a0cdc4a283eb717c54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
